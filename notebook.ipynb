{"cells":[{"attachments":{},"cell_type":"markdown","id":"prostate-arizona","metadata":{"id":"bA5ajAmk7XH6"},"source":["# 1. Introduction to Deep Learning"]},{"attachments":{},"cell_type":"markdown","id":"b947ad55-f8d5-4c81-a154-90678ab33b82","metadata":{},"source":["Deep learning is a subfield of machine learning that involves the creation and training of neural networks with multiple layers. Deep learning models are capable of automatically learning hierarchical representations of data, allowing them to perform complex tasks such as image and speech recognition, natural language processing, and autonomous driving. The networks consist of layers of nodes, or neurons, that process and transform input data to generate output predictions. The term \"deep\" refers to the fact that the networks can have many layers, which enables them to learn complex patterns and relationships in the data.\n","\n","While neural networks can have multiple layers, they are not necessarily deep. Deep learning, on the other hand, refers specifically to neural networks with multiple hidden layers that are used to model complex relationships between inputs and outputs.\n","\n","Neural networks are composed of various elements and functions that work together to make predictions and learn from data. Here are some of the key elements and functions:\n","- Input layer: This layer receives the input data, such as images, audio, or text.\n","- Hidden layers: These layers are in between the input and output layers, and they process the input data using weights and biases to produce output values.\n","- Output layer: This layer produces the final output of the network, such as a predicted label or a probability distribution over classes.\n","- Neurons: These are the basic units of a neural network that perform computations using inputs, weights, and biases. They are also known as nodes or units.\n","- Activation function: This function is applied to the output of each neuron to introduce non-linearity into the network and allow it to model complex relationships between inputs and outputs. Common activation functions include ReLU, sigmoid, and tanh.\n","- Weight: This is a parameter of a neuron that determines the strength of the connection between the neuron's inputs and its output. It is adjusted during training to improve the accuracy of the network's predictions.\n","- Bias: This is an additional parameter of a neuron that determines the neuron's output when all its inputs are zero. It is also adjusted during training.\n","- Loss function: This function measures the difference between the predicted output of the network and the true output, and it is used to train the network by adjusting the weights and biases to minimize the loss.\n","- Optimization algorithm: This algorithm is used to update the weights and biases during training to minimize the loss. Common optimization algorithms include stochastic gradient descent (SGD) and its variants, such as Adam and RMSprop.\n","  \n","By combining these elements and functions, neural networks can learn complex patterns and relationships in data and make accurate predictions on new data."]},{"attachments":{},"cell_type":"markdown","id":"73d70ac4-12e5-4a92-a57f-e77996dd910c","metadata":{},"source":["## Forward propagation algorithm"]},{"attachments":{},"cell_type":"markdown","id":"281be40b-3aca-43e7-998a-78e69853cfe6","metadata":{},"source":["Forward propagation is the process of computing the output of a neural network given an input. It involves passing the input through the network layer by layer, with each layer transforming the input in some way, until the output is produced."]},{"attachments":{},"cell_type":"markdown","id":"5769edcb-423d-46f5-9a60-cdc93d78aa93","metadata":{},"source":["![example_1](imgs/example_1.png)\n"]},{"cell_type":"code","execution_count":28,"id":"3e1707e6-9bb3-42b6-b355-28c8a788adfa","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"import numpy as np"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":29,"id":"9b83b607-dd4c-441e-b4fc-dbbc359e140c","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"input_data = np.array([2, 3])\nweights = {'node_0': np.array([1, 1]),'node_1': np.array([-1, 1]),'output': np.array([2, -1])}"},"outputs":[],"source":["input_data = np.array([2, 3])\n","weights = {'node_0': np.array([1, 1]),'node_1': np.array([-1, 1]),'output': np.array([2, -1])}"]},{"cell_type":"code","execution_count":30,"id":"c9ca0345-b549-45d8-8e0c-2ecf2b692744","metadata":{"executionTime":61,"lastSuccessfullyExecutedCode":"# Calculate node 0 value: node_0_value\nnode_0_value = (weights['node_0'] * input_data).sum()\n\n# Calculate node 1 value: node_1_value\nnode_1_value = (weights['node_1'] * input_data).sum()\n\n# Put node values into array: hidden_layer_outputs\nhidden_layer_outputs = np.array([node_0_value, node_1_value])\n\n# Calculate output: output\noutput = (weights['output'] * hidden_layer_outputs).sum()\n\n# Print output\nprint(output)"},"outputs":[],"source":["# Calculate node 0 value: node_0_value\n","node_0_value = (weights['node_0'] * input_data).sum()\n","\n","# Calculate node 1 value: node_1_value\n","node_1_value = (weights['node_1'] * input_data).sum()\n","\n","# Put node values into array: hidden_layer_outputs\n","hidden_layer_outputs = np.array([node_0_value, node_1_value])\n","\n","# Calculate output: output\n","output = (weights['output'] * hidden_layer_outputs).sum()\n","\n","# Print output\n","print(output)"]},{"attachments":{},"cell_type":"markdown","id":"d7823bd6-679d-4e7d-8dcb-035fde2090e5","metadata":{},"source":["## The Rectified Linear Activation Function"]},{"attachments":{},"cell_type":"markdown","id":"8bbede2c-3873-4277-a0da-1f1e54565ed5","metadata":{},"source":["Activation functions are used in neural networks to introduce nonlinearity into the output of a neuron. Without an activation function, a neural network would simply be a linear regression model, and would be limited in its ability to learn complex patterns in the data."]},{"attachments":{},"cell_type":"markdown","id":"7f5a7d98-71ea-4fba-884d-1ef0c7366b0a","metadata":{},"source":["The Rectified Linear Activation Function (ReLU) is an activation function, a piecewise linear function that returns the input if it is positive, and zero otherwise. In other words, ReLU is defined as: f(x) = max(0, x), where x is the input to the activation function, and f(x) is the output."]},{"cell_type":"code","execution_count":31,"id":"8fcc026a-0154-4c13-89cf-e976af879dbb","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"def relu(input):\n    \n    \"\"\"\n    Computes the rectified linear unit (ReLU) activation function.\n\n    Parameters\n    ----------\n    input : float\n        The input value to the ReLU function.\n\n    Returns\n    -------\n    float\n        The output value of the ReLU function, which is the maximum of the input\n        value and 0.\n\n    Notes\n    -----\n    The ReLU activation function is commonly used in neural networks to introduce\n    nonlinearity in the network. It returns the input value if it is positive,\n    and 0 otherwise.\n    \"\"\"\n    \n    # Calculate the value for the output of the relu function: output\n    output = max(0, input)\n    \n    # Return the value just calculated\n    return(output)"},"outputs":[],"source":["def relu(input):\n","    \n","    \"\"\"\n","    Computes the rectified linear unit (ReLU) activation function.\n","\n","    Parameters\n","    ----------\n","    input : float\n","        The input value to the ReLU function.\n","\n","    Returns\n","    -------\n","    float\n","        The output value of the ReLU function, which is the maximum of the input\n","        value and 0.\n","\n","    Notes\n","    -----\n","    The ReLU activation function is commonly used in neural networks to introduce\n","    nonlinearity in the network. It returns the input value if it is positive,\n","    and 0 otherwise.\n","    \"\"\"\n","    \n","    # Calculate the value for the output of the relu function: output\n","    output = max(0, input)\n","    \n","    # Return the value just calculated\n","    return(output)"]},{"cell_type":"code","execution_count":32,"id":"4b1d63c9-551a-45b4-991d-82b43a82dfb2","metadata":{"executionTime":54,"lastSuccessfullyExecutedCode":"# Calculate node 0 value: node_0_output\nnode_0_input = (input_data * weights['node_0']).sum()\nnode_0_output = relu(node_0_input)\n\n# Calculate node 1 value: node_1_output\nnode_1_input = (input_data * weights['node_1']).sum()\nnode_1_output = relu(node_1_input)\n\n# Put node values into array: hidden_layer_outputs\nhidden_layer_outputs = np.array([node_0_output, node_1_output])\n\n# Calculate model output (do not apply relu)\nmodel_output = (hidden_layer_outputs * weights['output']).sum()\n\n# Print model output\nprint(model_output)"},"outputs":[],"source":["# Calculate node 0 value: node_0_output\n","node_0_input = (input_data * weights['node_0']).sum()\n","node_0_output = relu(node_0_input)\n","\n","# Calculate node 1 value: node_1_output\n","node_1_input = (input_data * weights['node_1']).sum()\n","node_1_output = relu(node_1_input)\n","\n","# Put node values into array: hidden_layer_outputs\n","hidden_layer_outputs = np.array([node_0_output, node_1_output])\n","\n","# Calculate model output (do not apply relu)\n","model_output = (hidden_layer_outputs * weights['output']).sum()\n","\n","# Print model output\n","print(model_output)"]},{"attachments":{},"cell_type":"markdown","id":"2823a4ce-5160-4360-8cdb-25bef5023a66","metadata":{},"source":["## Applying the network to many observations/rows of data"]},{"cell_type":"code","execution_count":33,"id":"280b506b-4013-4737-82f0-804f561f77e4","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"# Define predict_with_network()\ndef predict_with_network_h1(input_data_row, weights):\n    \n    \"\"\"\n    Predicts the output of a neural network with a single hidden layer using the given input data row and weights.\n\n    Parameters\n    ----------\n    input_data_row : numpy.ndarray\n        An array containing the input data row of the neural network.\n    weights : dict\n        A dictionary containing the weights of the neural network, with keys 'node_0' and 'node_1' for the weights      of the hidden layer nodes, and key 'output' for the weights of the output layer.\n\n    Returns\n    -------\n    float\n        The predicted output of the neural network.\n\n    Notes\n    -----\n    The neural network model has one hidden layer with two nodes, and an output\n    layer with a single node. The rectified linear unit (ReLU) activation\n    function is applied to the output of each node in the network.\n    \"\"\"\n\n    # Calculate node 0 value\n    node_0_input = (input_data_row * weights['node_0']).sum()\n    node_0_output = relu(node_0_input)\n\n    # Calculate node 1 value\n    node_1_input = (input_data_row * weights['node_1']).sum()\n    node_1_output = relu(node_1_input)\n\n    # Put node values into array: hidden_layer_outputs\n    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n    \n    # Calculate model output\n    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n    model_output = relu(input_to_final_layer)\n    \n    # Return model output\n    return(model_output)"},"outputs":[],"source":["# Define predict_with_network()\n","def predict_with_network_h1(input_data_row, weights):\n","    \n","    \"\"\"\n","    Predicts the output of a neural network with a single hidden layer using the given input data row and weights.\n","\n","    Parameters\n","    ----------\n","    input_data_row : numpy.ndarray\n","        An array containing the input data row of the neural network.\n","    weights : dict\n","        A dictionary containing the weights of the neural network, with keys 'node_0' and 'node_1' for the weights      of the hidden layer nodes, and key 'output' for the weights of the output layer.\n","\n","    Returns\n","    -------\n","    float\n","        The predicted output of the neural network.\n","\n","    Notes\n","    -----\n","    The neural network model has one hidden layer with two nodes, and an output\n","    layer with a single node. The rectified linear unit (ReLU) activation\n","    function is applied to the output of each node in the network.\n","    \"\"\"\n","\n","    # Calculate node 0 value\n","    node_0_input = (input_data_row * weights['node_0']).sum()\n","    node_0_output = relu(node_0_input)\n","\n","    # Calculate node 1 value\n","    node_1_input = (input_data_row * weights['node_1']).sum()\n","    node_1_output = relu(node_1_input)\n","\n","    # Put node values into array: hidden_layer_outputs\n","    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n","    \n","    # Calculate model output\n","    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n","    model_output = relu(input_to_final_layer)\n","    \n","    # Return model output\n","    return(model_output)"]},{"cell_type":"code","execution_count":34,"id":"44eb685a-d2b7-4d44-b980-c442ece27009","metadata":{"executionTime":49,"lastSuccessfullyExecutedCode":"input_data = np.array([[3,5],[1,-1],[0,0],[8,4]])\nweights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n\n# Create empty list to store prediction results\nresults = []\nfor input_data_row in input_data:\n    # Append prediction to results\n    results.append(predict_with_network_h1(input_data_row, weights))\n\n# Print results\nprint(results)    "},"outputs":[],"source":["input_data = np.array([[3,5],[1,-1],[0,0],[8,4]])\n","weights = {'node_0': np.array([2, 4]), 'node_1': np.array([ 4, -5]), 'output': np.array([2, 7])}\n","\n","# Create empty list to store prediction results\n","results = []\n","for input_data_row in input_data:\n","    # Append prediction to results\n","    results.append(predict_with_network_h1(input_data_row, weights))\n","\n","# Print results\n","print(results)    "]},{"attachments":{},"cell_type":"markdown","id":"fd282ee0-0453-4414-a242-ae2eaa4ce75b","metadata":{},"source":["## Multi-layer neural networks"]},{"attachments":{},"cell_type":"markdown","id":"5c7d28bc-542f-4bba-97d0-ffd5f466c4ad","metadata":{},"source":["Multi-layer neural networks contain more than one layer of neurons between the input and output layers. These layers are called hidden layers, as their output is not directly observed.\n","\n","The hidden layers of a multi-layer neural network allow it to learn more complex representations of the input data, and thus to model more complex functions. Each hidden layer applies a non-linear transformation to the input data, and passes the result to the next layer. The output layer produces the final output of the network, which can be a classification, regression, or other kind of prediction."]},{"attachments":{},"cell_type":"markdown","id":"d9f797bd-e591-4f89-9aa9-78887f90989f","metadata":{},"source":["![example_2](imgs/example_2.png)\n"]},{"cell_type":"code","execution_count":35,"id":"3c320ffa-8258-4896-9938-753b335b28c8","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"def predict_with_network_h2(input_data, weights):\n    \n    \"\"\"\n    Computes the output of a neural network with two hidden layers and ReLU activation function.\n\n    Parameters\n    ----------\n    input_data_row : numpy.ndarray\n        An array containing the input data row of the neural network.\n    weights : dict\n        A dictionary containing the weights of the neural network.\n\n    Returns\n    -------\n    float\n        The predicted output of the neural network model.\n\n    Notes\n    -----\n    The neural network model has two hidden layers, each containing two nodes.\n    The output of the first layer is passed as input to the second layer, and\n    the output of the second layer is used to calculate the final output of the\n    network. The rectified linear unit (ReLU) activation function is applied to\n    the output of each node in the network.\n    \"\"\"\n    \n    # Calculate node 0 in the first hidden layer\n    node_0_0_input = (input_data * weights['node_0_0']).sum()\n    node_0_0_output = relu(node_0_0_input)\n\n    # Calculate node 1 in the first hidden layer\n    node_0_1_input = (input_data * weights['node_0_1']).sum()\n    node_0_1_output = relu(node_0_1_input)\n\n    # Put node values into array: hidden_0_outputs\n    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n    \n    # Calculate node 0 in the second hidden layer\n    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()\n    node_1_0_output = relu(node_1_0_input)\n\n    # Calculate node 1 in the second hidden layer\n    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n    node_1_1_output = relu(node_1_1_input)\n\n    # Put node values into array: hidden_1_outputs\n    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n\n    # Calculate model output: model_output\n    model_output = (hidden_1_outputs * weights['output']).sum()\n    \n    # Return model_output\n    return(model_output)"},"outputs":[],"source":["def predict_with_network_h2(input_data, weights):\n","    \n","    \"\"\"\n","    Computes the output of a neural network with two hidden layers and ReLU activation function.\n","\n","    Parameters\n","    ----------\n","    input_data_row : numpy.ndarray\n","        An array containing the input data row of the neural network.\n","    weights : dict\n","        A dictionary containing the weights of the neural network.\n","\n","    Returns\n","    -------\n","    float\n","        The predicted output of the neural network model.\n","\n","    Notes\n","    -----\n","    The neural network model has two hidden layers, each containing two nodes.\n","    The output of the first layer is passed as input to the second layer, and\n","    the output of the second layer is used to calculate the final output of the\n","    network. The rectified linear unit (ReLU) activation function is applied to\n","    the output of each node in the network.\n","    \"\"\"\n","    \n","    # Calculate node 0 in the first hidden layer\n","    node_0_0_input = (input_data * weights['node_0_0']).sum()\n","    node_0_0_output = relu(node_0_0_input)\n","\n","    # Calculate node 1 in the first hidden layer\n","    node_0_1_input = (input_data * weights['node_0_1']).sum()\n","    node_0_1_output = relu(node_0_1_input)\n","\n","    # Put node values into array: hidden_0_outputs\n","    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n","    \n","    # Calculate node 0 in the second hidden layer\n","    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()\n","    node_1_0_output = relu(node_1_0_input)\n","\n","    # Calculate node 1 in the second hidden layer\n","    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n","    node_1_1_output = relu(node_1_1_input)\n","\n","    # Put node values into array: hidden_1_outputs\n","    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n","\n","    # Calculate model output: model_output\n","    model_output = (hidden_1_outputs * weights['output']).sum()\n","    \n","    # Return model_output\n","    return(model_output)"]},{"cell_type":"code","execution_count":36,"id":"8704f000-6ff7-4f53-84d8-d12a23d2051c","metadata":{"executionTime":48,"lastSuccessfullyExecutedCode":"input_data = np.array([3,5])\nweights = {'node_0_0': np.array([2, 4]),\n 'node_0_1': np.array([ 4, -5]),\n 'node_1_0': np.array([-1,  2]),\n 'node_1_1': np.array([1, 2]),\n 'output': np.array([2, 7])}\n\noutput = predict_with_network_h2(input_data, weights)\nprint(output)"},"outputs":[],"source":["input_data = np.array([3,5])\n","weights = {'node_0_0': np.array([2, 4]),\n"," 'node_0_1': np.array([ 4, -5]),\n"," 'node_1_0': np.array([-1,  2]),\n"," 'node_1_1': np.array([1, 2]),\n"," 'output': np.array([2, 7])}\n","\n","output = predict_with_network_h2(input_data, weights)\n","print(output)"]},{"attachments":{},"cell_type":"markdown","id":"98852553-aa52-473b-a9e5-d043b0c2bab6","metadata":{},"source":["## How weight changes affect accuracy?"]},{"attachments":{},"cell_type":"markdown","id":"3fd8d236-c614-418f-9d18-4e6ebac62394","metadata":{},"source":["The weights in a neural network are the parameters that determine how the input data is transformed through the layers of the network to produce the output. When training a neural network, the goal is to find the optimal values for the weights that will produce accurate predictions on new, unseen data.\n","\n","If the weight changes are small and the network structure is well-suited to the problem at hand, then the changes are more likely to improve accuracy. On the other hand, if the weight changes are large or the network structure is not appropriate for the problem, then the changes may reduce accuracy or cause the network to converge to a suboptimal solution."]},{"attachments":{},"cell_type":"markdown","id":"314114cb-4eb4-4533-9e42-7868ed9f66c2","metadata":{},"source":["![example_3](imgs/example_3.png)\n"]},{"cell_type":"code","execution_count":37,"id":"c7a11861-f9d9-49f2-ad9b-2d1e901ca50b","metadata":{"executionTime":45,"lastSuccessfullyExecutedCode":"# The data point you will make a prediction for\ninput_data = np.array([0, 3])\n\n# Sample weights\nweights_0 = {'node_0': [2, 1],\n             'node_1': [1, 2],\n             'output': [1, 1]\n            }\n\n# The actual target value, used to calculate the error\ntarget_actual = 3\n\n# Make prediction using original weights\nmodel_output_0 = predict_with_network_h1(input_data, weights_0)\n\n# Calculate error: error_0\nerror_0 = model_output_0 - target_actual\n\n# Create weights that cause the network to make perfect prediction (3): weights_1\nweights_1 = {'node_0': [2, 1],\n             'node_1': [1, 2],\n             'output': [1, 0]\n            }\n\n# Make prediction using new weights: model_output_1\nmodel_output_1 = predict_with_network_h1(input_data, weights_1)\n\n# Calculate error: error_1\nerror_1 = model_output_1 - target_actual\n\n# Print error_0 and error_1\nprint(error_0)\nprint(error_1)"},"outputs":[],"source":["# The data point you will make a prediction for\n","input_data = np.array([0, 3])\n","\n","# Sample weights\n","weights_0 = {'node_0': [2, 1],\n","             'node_1': [1, 2],\n","             'output': [1, 1]\n","            }\n","\n","# The actual target value, used to calculate the error\n","target_actual = 3\n","\n","# Make prediction using original weights\n","model_output_0 = predict_with_network_h1(input_data, weights_0)\n","\n","# Calculate error: error_0\n","error_0 = model_output_0 - target_actual\n","\n","# Create weights that cause the network to make perfect prediction (3): weights_1\n","weights_1 = {'node_0': [2, 1],\n","             'node_1': [1, 2],\n","             'output': [1, 0]\n","            }\n","\n","# Make prediction using new weights: model_output_1\n","model_output_1 = predict_with_network_h1(input_data, weights_1)\n","\n","# Calculate error: error_1\n","error_1 = model_output_1 - target_actual\n","\n","# Print error_0 and error_1\n","print(error_0)\n","print(error_1)"]},{"attachments":{},"cell_type":"markdown","id":"47d0a10e-aa18-4dd7-97df-c7a8a30cd7ee","metadata":{},"source":["## Mean squared error"]},{"attachments":{},"cell_type":"markdown","id":"26333224-575b-493e-ad9f-e9c8153c66bf","metadata":{},"source":["In neural networks, Mean Squared Error (MSE) is a commonly used loss function to measure the difference between predicted and actual values. The MSE calculates the average squared difference between the predicted output and the actual output."]},{"attachments":{},"cell_type":"markdown","id":"8b277aa1-8cac-42ad-a2c6-04af62a7f79b","metadata":{},"source":["### Measure model accuracy on many points"]},{"cell_type":"code","execution_count":38,"id":"a1b65220-181d-44e4-a77b-3adeeda86835","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"input_data = np.array([[0,3],[1,2],[-1,-2],[4,0]])\ntarget_actuals = [1, 3, 5, 7]\nweights_0 = {'node_0': np.array([2, 1]), 'node_1': np.array([1, 2]), 'output': np.array([1, 1])}\nweights_1 = {'node_0': np.array([2, 1]),'node_1': np.array([1. , 1.5]),'output': np.array([1. , 1.5])}"},"outputs":[],"source":["input_data = np.array([[0,3],[1,2],[-1,-2],[4,0]])\n","target_actuals = [1, 3, 5, 7]\n","weights_0 = {'node_0': np.array([2, 1]), 'node_1': np.array([1, 2]), 'output': np.array([1, 1])}\n","weights_1 = {'node_0': np.array([2, 1]),'node_1': np.array([1. , 1.5]),'output': np.array([1. , 1.5])}"]},{"cell_type":"code","execution_count":39,"id":"43e66e22-2d58-4e98-9609-c3e0a15e8133","metadata":{"executionTime":44,"lastSuccessfullyExecutedCode":"from sklearn.metrics import mean_squared_error\n\n# Create model_output_0 \nmodel_output_0 = []\n# Create model_output_1\nmodel_output_1 = []\n\n# Loop over input_data\nfor row in input_data:\n    # Append prediction to model_output_0\n    model_output_0.append(predict_with_network_h1(row, weights_0))\n    \n    # Append prediction to model_output_1\n    model_output_1.append(predict_with_network_h1(row, weights_1))\n\n# Calculate the mean squared error for model_output_0: mse_0\nmse_0 = mean_squared_error(target_actuals, model_output_0)\n\n# Calculate the mean squared error for model_output_1: mse_1\nmse_1 = mean_squared_error(target_actuals, model_output_1)\n\n# Print mse_0 and mse_1\nprint(\"Mean squared error with weights_0: %f\" %mse_0)\nprint(\"Mean squared error with weights_1: %f\" %mse_1)"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error\n","\n","# Create model_output_0 \n","model_output_0 = []\n","# Create model_output_1\n","model_output_1 = []\n","\n","# Loop over input_data\n","for row in input_data:\n","    # Append prediction to model_output_0\n","    model_output_0.append(predict_with_network_h1(row, weights_0))\n","    \n","    # Append prediction to model_output_1\n","    model_output_1.append(predict_with_network_h1(row, weights_1))\n","\n","# Calculate the mean squared error for model_output_0: mse_0\n","mse_0 = mean_squared_error(target_actuals, model_output_0)\n","\n","# Calculate the mean squared error for model_output_1: mse_1\n","mse_1 = mean_squared_error(target_actuals, model_output_1)\n","\n","# Print mse_0 and mse_1\n","print(\"Mean squared error with weights_0: %f\" %mse_0)\n","print(\"Mean squared error with weights_1: %f\" %mse_1)"]},{"attachments":{},"cell_type":"markdown","id":"78d25bb8-d115-4db9-a6d0-edf45543627a","metadata":{},"source":["## Gradient descent for weights update"]},{"attachments":{},"cell_type":"markdown","id":"3297a8e0-4f37-4199-b47c-1cd14d2a3304","metadata":{},"source":["Gradient descent is an optimization algorithm used to minimize the cost or error function of a machine learning model. The cost function measures the difference between the predicted output and the actual output. The goal of gradient descent is to find the set of model parameters that minimize the cost function.\n","\n","In gradient descent, the model parameters are updated iteratively by subtracting a fraction of the gradient vector from the current parameter values. This fraction is known as the learning rate, and it determines the step size of the algorithm.\n","\n","The gradient descent algorithm moves in the direction of the negative gradient because the negative gradient points in the direction of steepest descent of the cost function. The magnitude of the gradient vector represents the slope of the cost function at a given point, and the direction of the gradient vector indicates the direction of the steepest slope.\n"]},{"attachments":{},"cell_type":"markdown","id":"d0183438-eedf-4ada-b87e-f7799b3fac91","metadata":{},"source":["In gradient descent, the weights or model parameters are updated iteratively based on the gradient of the cost function with respect to the weights. The update equation is as follows:\n","\n","θ = θ - α∇J(θ)\n","\n","where θ represents the vector of weights, α is the learning rate (a hyperparameter that determines the step size of the algorithm), J(θ) is the cost function, and ∇J(θ) is the gradient of the cost function with respect to the weights."]},{"attachments":{},"cell_type":"markdown","id":"86de859e-eef1-4c53-b270-5f4b990b33a4","metadata":{},"source":["### Example on how to imporve model weights using the gradient descent"]},{"cell_type":"code","execution_count":40,"id":"35fcba6d-a19e-4f02-85f3-136e57710149","metadata":{"executionTime":43,"lastSuccessfullyExecutedCode":"input_data = np.array([1,2,3])\nweights = np.array([0,2,1])\ntarget = 0\n\n# Set the learning rate: learning_rate\nlearning_rate = 0.01\n\n# Calculate the predictions: preds\npreds = (weights * input_data).sum()\n\n# Calculate the error: error\nerror = preds - target\n\n# Calculate the slope: slope\nslope = 2 * input_data * error\n\n# Update the weights: weights_updated\nweights_updated = weights-(slope*learning_rate)\n\n# Get updated predictions: preds_updated\npreds_updated = (weights_updated*input_data).sum()\n\n# Calculate updated error: error_updated\nerror_updated = preds_updated-target\n\n# Print the original error\nprint(error)\n\n# Print the updated error\nprint(error_updated)\n\n# Print the weights updated\nprint(weights_updated)"},"outputs":[],"source":["input_data = np.array([1,2,3])\n","weights = np.array([0,2,1])\n","target = 0\n","\n","# Set the learning rate: learning_rate\n","learning_rate = 0.01\n","\n","# Calculate the predictions: preds\n","preds = (weights * input_data).sum()\n","\n","# Calculate the error: error\n","error = preds - target\n","\n","# Calculate the slope: slope\n","slope = 2 * input_data * error\n","\n","# Update the weights: weights_updated\n","weights_updated = weights-(slope*learning_rate)\n","\n","# Get updated predictions: preds_updated\n","preds_updated = (weights_updated*input_data).sum()\n","\n","# Calculate updated error: error_updated\n","error_updated = preds_updated-target\n","\n","# Print the original error\n","print(error)\n","\n","# Print the updated error\n","print(error_updated)\n","\n","# Print the weights updated\n","print(weights_updated)"]},{"attachments":{},"cell_type":"markdown","id":"7b42ed99-cf59-4195-bf76-8d32103027af","metadata":{},"source":["## Slope and MSE functions"]},{"cell_type":"code","execution_count":41,"id":"a8433cb6-ebee-4a15-88be-bb055711cd03","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"def get_slope(input_data, target, weights):\n    \n    \"\"\"\n    Calculate the slope of the mean squared error with respect to the weights.\n\n    Parameters\n    ----------\n    input_data : array-like\n        The input data for the linear regression model.\n    target : array-like\n        The target values for the linear regression model.\n    weights : array-like\n        The weights to be used for the linear regression model.\n\n    Returns\n    -------\n    slope : ndarray\n        The slope of the mean squared error with respect to the weights.\n\n    Notes\n    -----\n    This function assumes that `input_data`, `target`, and `weights` are all one-dimensional arrays of the same length.\n    \"\"\"\n    \n    # Calculate the predictions: preds\n    preds = (weights * input_data).sum()\n    # Calculate the error: error\n    error = preds - target\n    # Calculate the slope: slope\n    slope = 2 * input_data * error\n    return slope"},"outputs":[],"source":["def get_slope(input_data, target, weights):\n","    \n","    \"\"\"\n","    Calculate the slope of the mean squared error with respect to the weights.\n","\n","    Parameters\n","    ----------\n","    input_data : array-like\n","        The input data for the linear regression model.\n","    target : array-like\n","        The target values for the linear regression model.\n","    weights : array-like\n","        The weights to be used for the linear regression model.\n","\n","    Returns\n","    -------\n","    slope : ndarray\n","        The slope of the mean squared error with respect to the weights.\n","\n","    Notes\n","    -----\n","    This function assumes that `input_data`, `target`, and `weights` are all one-dimensional arrays of the same length.\n","    \"\"\"\n","    \n","    # Calculate the predictions: preds\n","    preds = (weights * input_data).sum()\n","    # Calculate the error: error\n","    error = preds - target\n","    # Calculate the slope: slope\n","    slope = 2 * input_data * error\n","    return slope"]},{"cell_type":"code","execution_count":42,"id":"6e93172e-1fb8-4004-b3eb-f5e04eaa2337","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"def get_mse(input_data, target, weights):\n    \"\"\"\n    Calculate the mean squared error for a linear regression model.\n\n    Parameters\n    ----------\n    input_data : array-like\n        The input data for the linear regression model.\n    target : array-like\n        The target values for the linear regression model.\n    weights : array-like\n        The weights to be used for the linear regression model.\n\n    Returns\n    -------\n    mse : float\n        The mean squared error for the linear regression model.\n\n    Notes\n    -----\n    This function assumes that `input_data`, `target`, and `weights` are all one-dimensional arrays of the same length.\n    \"\"\"\n    # Calculate the predictions: preds\n    preds = (weights * input_data).sum()\n\n    # Calculate the error: error\n    error = preds - target\n\n    # Calculate the mean squared error: mse\n    mse = (error ** 2).mean()\n\n    return mse"},"outputs":[],"source":["def get_mse(input_data, target, weights):\n","    \"\"\"\n","    Calculate the mean squared error for a linear regression model.\n","\n","    Parameters\n","    ----------\n","    input_data : array-like\n","        The input data for the linear regression model.\n","    target : array-like\n","        The target values for the linear regression model.\n","    weights : array-like\n","        The weights to be used for the linear regression model.\n","\n","    Returns\n","    -------\n","    mse : float\n","        The mean squared error for the linear regression model.\n","\n","    Notes\n","    -----\n","    This function assumes that `input_data`, `target`, and `weights` are all one-dimensional arrays of the same length.\n","    \"\"\"\n","    # Calculate the predictions: preds\n","    preds = (weights * input_data).sum()\n","\n","    # Calculate the error: error\n","    error = preds - target\n","\n","    # Calculate the mean squared error: mse\n","    mse = (error ** 2).mean()\n","\n","    return mse"]},{"attachments":{},"cell_type":"markdown","id":"b4b42ab3-1a16-4ec5-81b0-c63273a22ae7","metadata":{},"source":["### Example of making multiple updates to weights"]},{"cell_type":"code","execution_count":43,"id":"680e4716-9aa3-48d7-8a34-aca527714ed6","metadata":{"executionTime":48,"lastSuccessfullyExecutedCode":"n_updates = 20\nmse_hist = []\nlearning_rate = 0.01\n\n# Iterate over the number of updates\nfor i in range(n_updates):\n    # Calculate the slope: slope\n    slope = get_slope(input_data, target, weights)\n    \n    # Update the weights: weights\n    weights = weights - learning_rate * slope\n    \n    # Calculate mse with new weights: mse\n    mse = get_mse(input_data, target, weights)\n    \n    # Append the mse to mse_hist\n    mse_hist.append(mse)\n\n# Plot the mse history\nimport matplotlib.pyplot as plt\nplt.plot(mse_hist)\nplt.xlabel('Iterations')\nplt.ylabel('Mean Squared Error')\nplt.show()"},"outputs":[],"source":["n_updates = 20\n","mse_hist = []\n","learning_rate = 0.01\n","\n","# Iterate over the number of updates\n","for i in range(n_updates):\n","    # Calculate the slope: slope\n","    slope = get_slope(input_data, target, weights)\n","    \n","    # Update the weights: weights\n","    weights = weights - learning_rate * slope\n","    \n","    # Calculate mse with new weights: mse\n","    mse = get_mse(input_data, target, weights)\n","    \n","    # Append the mse to mse_hist\n","    mse_hist.append(mse)\n","\n","# Plot the mse history\n","import matplotlib.pyplot as plt\n","plt.plot(mse_hist)\n","plt.xlabel('Iterations')\n","plt.ylabel('Mean Squared Error')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","id":"ca1914c2-9bce-41cf-8213-2e84dde8436d","metadata":{},"source":["## Backpropagation"]},{"attachments":{},"cell_type":"markdown","id":"76860e10-cf4c-4acf-b351-0181ffee128d","metadata":{},"source":["![backpropagation](imgs/backpropagation.png)\n"]},{"attachments":{},"cell_type":"markdown","id":"9458fd81-62ad-4da6-99d3-636ed3600991","metadata":{},"source":["Backpropagation is a process that allows neural networks to learn from data by iteratively adjusting the weights and biases of the neurons in the network. The goal of backpropagation is to minimize the difference between the network's predicted output and the actual output, which is often represented by a cost or loss function.\n","\n","The algorithm involves propagating the error or loss of the network backwards from the output layer to the input layer, and adjusting the weights of the neurons in the network based on the gradient of the error with respect to the weights."]},{"attachments":{},"cell_type":"markdown","id":"e2b9b98f-916b-4282-a695-85c724227fc2","metadata":{},"source":["### Backpropagation process "]},{"attachments":{},"cell_type":"markdown","id":"9989ff46-2e03-4dac-8b8d-abc1f7311d04","metadata":{},"source":["Here is a step-by-step overview of the backpropagation process:\n","\n","Forward Propagation: The forward propagation step is the process of passing input data through the network to obtain an output prediction. During this step, each neuron in the network calculates a weighted sum of the inputs it receives, adds a bias term, and applies an activation function to produce an output value.\n","\n","Calculate Error: After obtaining the output prediction, we calculate the error or loss of the network by comparing the predicted output to the actual output.\n","\n","Backward Propagation: During this step, the error is propagated backwards through the network, starting from the output layer and moving towards the input layer. The gradient of the error with respect to the weights of each neuron in the network is calculated using the chain rule of calculus.\n","\n","Update Weights: Once the gradient has been calculated, it is used to update the weights of each neuron in the network. This update is performed using an optimization algorithm such as stochastic gradient descent, which adjusts the weights in the direction that reduces the error.\n","\n","Repeat: Steps 1-4 are repeated for each input in the training set until the network converges to a set of weights that minimizes the error."]},{"attachments":{},"cell_type":"markdown","id":"94a5aa90-8721-4abf-9525-c09f6d4367d9","metadata":{},"source":["### Example of backpropagation function"]},{"attachments":{},"cell_type":"markdown","id":"46f3aa49-4df5-49bb-8128-3061fbccf28c","metadata":{},"source":["The function takes as input the training data X and labels y, the number of hidden units n_hidden, the learning rate, and the number of training iterations. It initializes the weights randomly and performs a loop over the training iterations.\n","\n","In each iteration, it performs forward propagation to compute the network output y_pred, and then computes the mean squared error loss and gradients using backpropagation. It then updates the weights using the gradients and the learning rate. Finally, it prints the progress and returns the final weights W and b.\n","\n"]},{"cell_type":"code","execution_count":44,"id":"e4b4398e-942c-4856-8fc1-4949494e7bd9","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"import numpy as np\n\ndef backpropagation_single_layer(X, y, n_hidden, learning_rate, n_iterations):\n    # Initialize weights and biases\n    W = np.random.randn(X.shape[1], n_hidden)\n    b = np.zeros((1, n_hidden))\n\n    # Initialize list to store MSE history\n    mse_hist = []\n\n    # Gradient descent loop\n    for i in range(n_iterations):\n        # Forward propagation\n        z = np.dot(X, W) + b\n        a = 1 / (1 + np.exp(-z))\n\n        # Backpropagation\n        dz = a - y\n        dW = np.dot(X.T, dz)\n        db = np.sum(dz, axis=0, keepdims=True)\n\n        # Update weights and biases\n        W -= learning_rate * dW\n        b -= learning_rate * db\n\n        # Compute and store MSE\n        mse = np.mean(np.square(a - y))\n        mse_hist.append(mse)\n\n    return W, b, mse_hist\n"},"outputs":[],"source":["import numpy as np\n","\n","def backpropagation_single_layer(X, y, n_hidden, learning_rate, n_iterations):\n","    # Initialize weights and biases\n","    W = np.random.randn(X.shape[1], n_hidden)\n","    b = np.zeros((1, n_hidden))\n","\n","    # Initialize list to store MSE history\n","    mse_hist = []\n","\n","    # Gradient descent loop\n","    for i in range(n_iterations):\n","        # Forward propagation\n","        z = np.dot(X, W) + b\n","        a = 1 / (1 + np.exp(-z))\n","\n","        # Backpropagation\n","        dz = a - y\n","        dW = np.dot(X.T, dz)\n","        db = np.sum(dz, axis=0, keepdims=True)\n","\n","        # Update weights and biases\n","        W -= learning_rate * dW\n","        b -= learning_rate * db\n","\n","        # Compute and store MSE\n","        mse = np.mean(np.square(a - y))\n","        mse_hist.append(mse)\n","\n","    return W, b, mse_hist\n"]},{"cell_type":"code","execution_count":45,"id":"4fad5301-3db5-4b6c-81cf-491e455ffa5f","metadata":{"executionTime":39,"lastSuccessfullyExecutedCode":"# Generate some sample data\nX = np.random.randn(100, 2)\ny = np.random.randint(0, 2, size=(100, 1))\n\n# Train a single-layer network using backpropagation\nn_hidden = 4\nlearning_rate = 0.1\nn_iterations = 20\nW, b, mse_hist = backpropagation_single_layer(X, y, n_hidden, learning_rate, n_iterations)\n\n# Print the final weights and biases\nprint(\"Final weights:\")\nprint(W)\nprint(\"Final biases:\")\nprint(b)\n\n# Plot the MSE history\nimport matplotlib.pyplot as plt\nplt.plot(mse_hist)\nplt.xlabel('Iterations')\nplt.ylabel('Mean Squared Error')\nplt.show()\n"},"outputs":[],"source":["# Generate some sample data\n","X = np.random.randn(100, 2)\n","y = np.random.randint(0, 2, size=(100, 1))\n","\n","# Train a single-layer network using backpropagation\n","n_hidden = 4\n","learning_rate = 0.1\n","n_iterations = 20\n","W, b, mse_hist = backpropagation_single_layer(X, y, n_hidden, learning_rate, n_iterations)\n","\n","# Print the final weights and biases\n","print(\"Final weights:\")\n","print(W)\n","print(\"Final biases:\")\n","print(b)\n","\n","# Plot the MSE history\n","import matplotlib.pyplot as plt\n","plt.plot(mse_hist)\n","plt.xlabel('Iterations')\n","plt.ylabel('Mean Squared Error')\n","plt.show()\n"]},{"attachments":{},"cell_type":"markdown","id":"89327934-5eef-4bab-b03b-e6ab034d8361","metadata":{},"source":["# 2. Keras model"]},{"attachments":{},"cell_type":"markdown","id":"e5d16421-b274-4df6-b700-736d13afb630","metadata":{},"source":["Keras Sequential is a model type in the Keras API for building neural network models in Python. It allows you to create a model by simply adding layers to it one at a time, in a linear, sequential order.\n","\n","In Keras, a dense layer is a type of neural network layer that is also known as a fully connected layer. A dense layer takes in inputs from all the neurons in the previous layer and applies a set of weights to those inputs to produce a set of outputs. Each neuron in the dense layer is connected to every neuron in the previous layer. The number of neurons in the dense layer determines the number of outputs produced by that layer.\n","\n","The units parameter specifies the number of neurons in the layer. The activation parameter specifies the activation function to be used in the layer. The input_shape parameter specifies the shape of the input data."]},{"attachments":{},"cell_type":"markdown","id":"ff521e77-2120-4c2e-8491-0049946e41f0","metadata":{},"source":["### Prepare the data"]},{"cell_type":"code","execution_count":46,"id":"6c2d2b13-0c13-4dec-87e2-cd823e499b0c","metadata":{"executionTime":42,"lastSuccessfullyExecutedCode":"# Dataframe\nimport pandas as pd\ndf = pd.read_csv(\"datasets/hourly_wages.csv\")\ntarget = df['wage_per_hour'].to_numpy()\ndfp = df.drop(['wage_per_hour'], axis=1)\npredictors = dfp.to_numpy()"},"outputs":[],"source":["# Dataframe\n","import pandas as pd\n","df = pd.read_csv(\"datasets/hourly_wages.csv\")\n","target = df['wage_per_hour'].to_numpy()\n","dfp = df.drop(['wage_per_hour'], axis=1)\n","predictors = dfp.to_numpy()"]},{"attachments":{},"cell_type":"markdown","id":"03fa10fa-03db-4191-9887-cb511279576d","metadata":{},"source":["### Specify a model"]},{"cell_type":"code","execution_count":47,"id":"0bc83eef-8458-49a2-ae49-a61778e379f4","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"# Import necessary modules\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\n\n# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\n\n# Set up the model: model\nmodel = Sequential()\n\n# Add the first layer\nmodel.add(Dense(50, activation='relu', input_shape = (n_cols,)))\n\n# Add the second layer\nmodel.add(Dense(32, activation='relu'))\n\n# Add the output layer\nmodel.add(Dense(1))"},"outputs":[],"source":["# Import necessary modules\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","\n","# Save the number of columns in predictors: n_cols\n","n_cols = predictors.shape[1]\n","\n","# Set up the model: model\n","model = Sequential()\n","\n","# Add the first layer\n","model.add(Dense(50, activation='relu', input_shape = (n_cols,)))\n","\n","# Add the second layer\n","model.add(Dense(32, activation='relu'))\n","\n","# Add the output layer\n","model.add(Dense(1))"]},{"attachments":{},"cell_type":"markdown","id":"37df0fe3-1d4e-4632-a067-3e760a514731","metadata":{},"source":["### Compiling the model"]},{"attachments":{},"cell_type":"markdown","id":"dfbdb18c-f27c-4514-8169-9f3b7c3ce23d","metadata":{},"source":["To compile the model, you need to specify the optimizer and loss function to use.\n","The Adam optimizer is an adaptive learning rate optimization algorithm that is well-suited for stochastic gradient descent problems, particularly those with large datasets or many parameters."]},{"cell_type":"code","execution_count":48,"id":"af4e5da3-c75f-44ca-b40a-d88e919c85e5","metadata":{"executionTime":34,"lastSuccessfullyExecutedCode":"# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\n# Verify that model contains information from compiling\nprint(\"Loss function: \" + model.loss)"},"outputs":[],"source":["# Compile the model\n","model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Verify that model contains information from compiling\n","print(\"Loss function: \" + model.loss)"]},{"attachments":{},"cell_type":"markdown","id":"bd8eb3fc-cd01-4058-b4a9-89ba105e7cd6","metadata":{},"source":["### Fitting the model"]},{"cell_type":"code","execution_count":49,"id":"20dd2270-0ebb-489d-8d87-090774244543","metadata":{"executionTime":48,"lastSuccessfullyExecutedCode":"model.fit(predictors, target)"},"outputs":[],"source":["model.fit(predictors, target)"]},{"attachments":{},"cell_type":"markdown","id":"5d7c4352-f62e-4a2b-9fbf-d7b3c1edff6b","metadata":{},"source":["## Classification model using the titanic dataset"]},{"attachments":{},"cell_type":"markdown","id":"caa3ecc1-02f3-425c-b9fc-fd964501a268","metadata":{},"source":["An example of how to use Keras to create a neural network model for binary classification, and then use the trained model to make predictions on new data."]},{"cell_type":"code","execution_count":50,"id":"a706fd6f-5182-455d-9070-c33441e77052","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"# Dataframe\nimport pandas as pd\ndf = pd.read_csv(\"datasets/titanic_all_numeric.csv\")\ndfn = df.drop(['survived'], axis=1)\npredictors = dfn.to_numpy().astype(np.float32)\nn_cols = dfn.shape[1]"},"outputs":[],"source":["# Dataframe\n","import pandas as pd\n","df = pd.read_csv(\"datasets/titanic_all_numeric.csv\")\n","dfn = df.drop(['survived'], axis=1)\n","predictors = dfn.to_numpy().astype(np.float32)\n","n_cols = dfn.shape[1]"]},{"cell_type":"code","execution_count":51,"id":"4d4bf35a-2e10-4a0d-9901-c731d718568e","metadata":{"executionTime":85,"lastSuccessfullyExecutedCode":"# Import necessary modules\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\n\n# Convert the target to categorical: target\ntarget = to_categorical(df.survived.astype(np.float32))\n\n# Set up the model\nmodel = Sequential()\n\n# Add the first layer\nmodel.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n\n# Add the output layer\nmodel.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='sgd', \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(predictors, target)"},"outputs":[],"source":["# Import necessary modules\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","\n","# Convert the target to categorical: target\n","target = to_categorical(df.survived.astype(np.float32))\n","\n","# Set up the model\n","model = Sequential()\n","\n","# Add the first layer\n","model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n","\n","# Add the output layer\n","model.add(Dense(2, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='sgd', \n","              loss='categorical_crossentropy', \n","              metrics=['accuracy'])\n","\n","# Fit the model\n","model.fit(predictors, target)"]},{"attachments":{},"cell_type":"markdown","id":"9df4487f-c0b9-4df4-a411-477078ed3455","metadata":{},"source":["Note: \n","\n","The Rectified Linear Activation Function (ReLU) is an activation function, a piecewise linear function that returns the input if it is positive, and zero otherwise. In other words, ReLU is defined as: f(x) = max(0, x), where x is the input to the activation function, and f(x) is the output.\n","\n","The softmax activation function is commonly used in the output layer of a neural network for multi-class classification problems. It takes a vector of real numbers as input and outputs another vector of the same dimension, where each element is a probability value between 0 and 1. The output vector is normalized so that its elements sum to 1, which makes it suitable for interpreting the output as a probability distribution over the classes.\n","\n","In SGD, instead of computing the gradient of the loss function with respect to the weights using the entire training dataset, the gradient is computed for a single training example at a time. This makes SGD faster and more efficient, especially for large datasets.\n","\n","Categorical_crossentropy is a commonly used loss function in neural networks for multi-class classification problems, where the output has a probability distribution over multiple classes. It is often used in conjunction with the softmax activation function in the output layer. The categorical cross-entropy loss function measures the dissimilarity between the true class distribution and the predicted class distribution."]},{"attachments":{},"cell_type":"markdown","id":"f428e29c-a72b-4483-ac40-b4e1cf29569a","metadata":{},"source":["### Making predictions"]},{"cell_type":"code","execution_count":52,"id":"7fa1cc85-39e7-4065-a471-205e56f49d86","metadata":{"executionTime":39,"lastSuccessfullyExecutedCode":"pred_data = np.array([[  2.      ,  34.      ,   0.      ,   0.      ,  13.      ,\n          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n       [  2.      ,  31.      ,   1.      ,   1.      ,  26.25    ,\n          0.      ,   0.      ,   0.      ,   0.      ,   1.      ]])\n                      \n# Calculate predictions: predictions\npredictions = model.predict(pred_data)\n\n# Calculate predicted probability of survival: predicted_prob_true\npredicted_prob_true = predictions[:,1]\n\n# Print predicted_prob_true\nprint(predicted_prob_true)"},"outputs":[],"source":["pred_data = np.array([[  2.      ,  34.      ,   0.      ,   0.      ,  13.      ,\n","          1.      ,   0.      ,   0.      ,   0.      ,   1.      ],\n","       [  2.      ,  31.      ,   1.      ,   1.      ,  26.25    ,\n","          0.      ,   0.      ,   0.      ,   0.      ,   1.      ]])\n","                      \n","# Calculate predictions: predictions\n","predictions = model.predict(pred_data)\n","\n","# Calculate predicted probability of survival: predicted_prob_true\n","predicted_prob_true = predictions[:,1]\n","\n","# Print predicted_prob_true\n","print(predicted_prob_true)"]},{"attachments":{},"cell_type":"markdown","id":"000c35d2-f165-4a04-8817-fe9ac4f6c3e3","metadata":{},"source":["Note: The predicted probabilities of survival for the new data are stored in the predicted_prob_true."]},{"attachments":{},"cell_type":"markdown","id":"441b091d-2478-4a48-a678-65a76e2941b0","metadata":{},"source":["### Changing optimization parameters"]},{"attachments":{},"cell_type":"markdown","id":"b480ce81-17d7-4b24-a5ce-e2cc4245da49","metadata":{},"source":["Optimize a model at a very low learning rate, a very high learning rate, and a \"just right\" learning rate. "]},{"cell_type":"code","execution_count":53,"id":"fa3e91b1-38a2-4d1f-9a54-e51a8d4a886d","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"def get_new_model(input_shape):\n    model = Sequential()\n    model.add(Dense(100, activation='relu', input_shape = input_shape))\n    model.add(Dense(100, activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n    return(model)"},"outputs":[],"source":["def get_new_model(input_shape):\n","    model = Sequential()\n","    model.add(Dense(100, activation='relu', input_shape = input_shape))\n","    model.add(Dense(100, activation='relu'))\n","    model.add(Dense(2, activation='softmax'))\n","    return(model)"]},{"cell_type":"code","execution_count":54,"id":"1bbbeba6-3ae4-4b1d-9655-7820f6451dd0","metadata":{"executionTime":62,"lastSuccessfullyExecutedCode":"input_shape=(n_cols,)\n\n# Import the SGD optimizer\nfrom tensorflow.keras.optimizers import SGD\n\n# Create list of learning rates: lr_to_test\nlr_to_test = [0.000001, 0.01, 1]\n\n# Loop over learning rates\nfor lr in lr_to_test:\n    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n    \n    # Build new model to test, unaffected by previous models\n    model = get_new_model(input_shape)\n    \n    # Create SGD optimizer with specified learning rate: my_optimizer\n    my_optimizer = SGD(lr=lr)\n    \n    # Compile the model\n    model.compile(optimizer = 'adam', loss ='categorical_crossentropy')\n    \n    # Fit the model\n    model.fit(predictors, target)"},"outputs":[],"source":["input_shape=(n_cols,)\n","\n","# Import the SGD optimizer\n","from tensorflow.keras.optimizers import SGD\n","\n","# Create list of learning rates: lr_to_test\n","lr_to_test = [0.000001, 0.01, 1]\n","\n","# Loop over learning rates\n","for lr in lr_to_test:\n","    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n","    \n","    # Build new model to test, unaffected by previous models\n","    model = get_new_model(input_shape)\n","    \n","    # Create SGD optimizer with specified learning rate: my_optimizer\n","    my_optimizer = SGD(lr=lr)\n","    \n","    # Compile the model\n","    model.compile(optimizer = 'adam', loss ='categorical_crossentropy')\n","    \n","    # Fit the model\n","    model.fit(predictors, target)"]},{"attachments":{},"cell_type":"markdown","id":"47daf591-4951-463e-8471-01669a94e323","metadata":{},"source":["### Evaluating model accuracy on validation dataset. Optimizing the optimization."]},{"attachments":{},"cell_type":"markdown","id":"af00803c-02bc-478a-ba33-9876e8c17603","metadata":{},"source":["In Keras, EarlyStopping is a callback that stops the training of a neural network model when a monitored metric has stopped improving. It helps to avoid overfitting and save time by stopping the training process early."]},{"cell_type":"code","execution_count":55,"id":"61f85d7f-d496-4daf-91d6-2601fec70898","metadata":{"executionTime":509,"lastSuccessfullyExecutedCode":"# Import EarlyStopping\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Save the number of columns in predictors: n_cols\nn_cols = predictors.shape[1]\ninput_shape = (n_cols,)\n\n# Specify the model\nmodel = Sequential()\nmodel.add(Dense(100, activation='relu', input_shape = input_shape))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer = 'adam', loss ='categorical_crossentropy', metrics=['accuracy'])\n\n# Define early_stopping_monitor\nearly_stopping_monitor = EarlyStopping(monitor='val_loss', patience=2)\n\n# Fit the model\nmodel.fit(predictors, target, validation_split=0.3,epochs=30, callbacks=[early_stopping_monitor])"},"outputs":[],"source":["# Import EarlyStopping\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Save the number of columns in predictors: n_cols\n","n_cols = predictors.shape[1]\n","input_shape = (n_cols,)\n","\n","# Specify the model\n","model = Sequential()\n","model.add(Dense(100, activation='relu', input_shape = input_shape))\n","model.add(Dense(100, activation='relu'))\n","model.add(Dense(2, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer = 'adam', loss ='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Define early_stopping_monitor\n","early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=2)\n","\n","# Fit the model\n","model.fit(predictors, target, validation_split=0.3,epochs=30, callbacks=[early_stopping_monitor])"]},{"attachments":{},"cell_type":"markdown","id":"2faa6cae-6627-4915-a41e-6cd41c1efe75","metadata":{},"source":["### Experimenting with wider networks"]},{"cell_type":"code","execution_count":56,"id":"2a2dfa67-d661-4025-9ce0-d9d2d6fb254d","metadata":{"executionTime":1709,"lastSuccessfullyExecutedCode":"from keras.models import Sequential\nfrom keras.layers import Dense\n\n# Define a narrow model\nnarrow_model = Sequential()\nnarrow_model.add(Dense(10, activation='relu', input_shape=input_shape))\nnarrow_model.add(Dense(10, activation='relu'))\nnarrow_model.add(Dense(2, activation='softmax'))\nnarrow_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the narrow model\nmodel_n_training = narrow_model.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n\n# Define a wider model\nwider_model = Sequential()\nwider_model.add(Dense(100, activation='relu', input_shape=input_shape))\nwider_model.add(Dense(100, activation='relu'))\nwider_model.add(Dense(2, activation='softmax'))\nwider_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the wider model\nmodel_w_training = wider_model.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n\n# Create the plot\nplt.plot(model_n_training.history['val_loss'], 'r', model_w_training.history['val_loss'], 'b')\nplt.xlabel('Epochs')\nplt.ylabel('Validation score')\nplt.show()"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","\n","# Define a narrow model\n","narrow_model = Sequential()\n","narrow_model.add(Dense(10, activation='relu', input_shape=input_shape))\n","narrow_model.add(Dense(10, activation='relu'))\n","narrow_model.add(Dense(2, activation='softmax'))\n","narrow_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the narrow model\n","model_n_training = narrow_model.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n","\n","# Define a wider model\n","wider_model = Sequential()\n","wider_model.add(Dense(100, activation='relu', input_shape=input_shape))\n","wider_model.add(Dense(100, activation='relu'))\n","wider_model.add(Dense(2, activation='softmax'))\n","wider_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the wider model\n","model_w_training = wider_model.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n","\n","# Create the plot\n","plt.plot(model_n_training.history['val_loss'], 'r', model_w_training.history['val_loss'], 'b')\n","plt.xlabel('Epochs')\n","plt.ylabel('Validation score')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","id":"650ccd66-8831-49f1-a192-5b7112567edd","metadata":{},"source":["Note: Notice the keyword argument verbose=False in model.fit(): This prints out fewer updates, since you'll be evaluating the models graphically instead of through text."]},{"attachments":{},"cell_type":"markdown","id":"becb73f5-d8e7-493c-8579-09f5ff037251","metadata":{},"source":["### Experimenting with layers"]},{"cell_type":"code","execution_count":57,"id":"5074f027-b023-446b-97b3-04b28103b56d","metadata":{"executionTime":1957,"lastSuccessfullyExecutedCode":"from keras.models import Sequential\nfrom keras.layers import Dense\n\n# Define a narrow model\nnarrow_model = Sequential()\nnarrow_model.add(Dense(10, activation='relu', input_shape=input_shape))\nnarrow_model.add(Dense(2, activation='softmax'))\nnarrow_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the narrow model\nmodel_n_training = narrow_model.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n\n# Define a wider model\nwider_model = Sequential()\nwider_model.add(Dense(10, activation='relu', input_shape=input_shape))\nwider_model.add(Dense(10, activation='relu'))\nwider_model.add(Dense(10, activation='relu'))\nwider_model.add(Dense(2, activation='softmax'))\nwider_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the wider model\nmodel_w_training = wider_model.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n\n# Create the plot\nplt.plot(model_n_training.history['val_loss'], 'r', model_w_training.history['val_loss'], 'b')\nplt.xlabel('Epochs')\nplt.ylabel('Validation score')\nplt.show()"},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","\n","# Define a narrow model\n","narrow_model = Sequential()\n","narrow_model.add(Dense(10, activation='relu', input_shape=input_shape))\n","narrow_model.add(Dense(2, activation='softmax'))\n","narrow_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the narrow model\n","model_n_training = narrow_model.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n","\n","# Define a wider model\n","wider_model = Sequential()\n","wider_model.add(Dense(10, activation='relu', input_shape=input_shape))\n","wider_model.add(Dense(10, activation='relu'))\n","wider_model.add(Dense(10, activation='relu'))\n","wider_model.add(Dense(2, activation='softmax'))\n","wider_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the wider model\n","model_w_training = wider_model.fit(predictors, target, epochs=15, validation_split=0.2, callbacks=[early_stopping_monitor], verbose=False)\n","\n","# Create the plot\n","plt.plot(model_n_training.history['val_loss'], 'r', model_w_training.history['val_loss'], 'b')\n","plt.xlabel('Epochs')\n","plt.ylabel('Validation score')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","id":"f01f054a-e0b1-49f2-97d3-44f1ea5b1a9e","metadata":{},"source":["### Model capacity"]},{"attachments":{},"cell_type":"markdown","id":"4cbe0ddb-089b-4f0d-acb7-ad2a09a01cee","metadata":{},"source":["Model capacity refers to the ability of a model to learn complex relationships between input and output data.\n","Sequential experiments involve progressively increasing the capacity of a model by adding layers or increasing the number of nodes in existing layers, in order to achieve better performance on a given task."]},{"attachments":{},"cell_type":"markdown","id":"35804e1a-e363-4dd6-b0a7-542a98ed2e3b","metadata":{},"source":["Here are some tips for conducting sequential experiments to optimize model capacity using Keras:\n","\n","- Start with a simple model: Begin with a simple model architecture that can solve the problem to some extent. This will serve as a baseline and help you evaluate whether adding complexity to the model improves performance.\n","- Use appropriate loss function and optimizer: Choose a suitable loss function and optimizer based on the problem you are solving. For instance, for binary classification problems, binary cross-entropy loss and Adam optimizer can be a good starting point.\n","- Adjust the number of neurons and layers: Experiment with different numbers of neurons and layers to find the optimal architecture for your problem. Use small increments in the number of neurons or layers and observe the effect on model performance.\n","- Change activation functions: Try different activation functions such as ReLU, sigmoid, or tanh. The choice of activation function can have a significant impact on model performance.\n","- Regularize the model: Use regularization techniques such as L1, L2, or dropout to prevent overfitting. Overfitting can occur when the model becomes too complex and starts fitting the noise in the training data.\n","- Use early stopping: Monitor the model's performance on the validation set and use early stopping to prevent the model from overfitting. Early stopping can be achieved by monitoring the validation loss and stopping training when the loss stops improving.\n","- Experiment with batch size and learning rate: Try different batch sizes and learning rates to find the optimal values. A small batch size can lead to faster convergence but can be computationally expensive. A large learning rate can result in overshooting the optimal solution, while a small learning rate can take longer to converge. \n","- Evaluate the model on a test set: Once you have found the optimal model architecture, evaluate it on a test set to get a final estimate of model performance. This will give you an idea of how well the model generalizes to unseen data.\n","\n","Remember that optimizing model capacity is an iterative process that requires experimentation and tuning. Keep a record of your experiments and their results, and use them to guide your decisions in future experiments."]},{"attachments":{},"cell_type":"markdown","id":"dbcd7e13-1b5b-450a-bb5d-4ee49671403c","metadata":{},"source":["## Application: Building a digit recognition model"]},{"cell_type":"code","execution_count":58,"id":"5158694a-c839-43f2-b789-3f253ab8330d","metadata":{"chartConfig":{"bar":{"hasRoundedCorners":true,"stacked":false},"type":"line","version":"v1"},"executionTime":1768,"lastSuccessfullyExecutedCode":"from mlxtend.data import mnist_data\nX, y = mnist_data()","visualizeDataframe":true},"outputs":[],"source":["from mlxtend.data import mnist_data\n","X, y = mnist_data()"]},{"cell_type":"code","execution_count":59,"id":"db2ac353-fb23-49c8-95a7-b5a730fc03e8","metadata":{"executionTime":0,"lastSuccessfullyExecutedCode":"from keras.utils import to_categorical\n\n# Convert labels to one-hot encoded representation\ny = to_categorical(y, num_classes=10)"},"outputs":[],"source":["from keras.utils import to_categorical\n","\n","# Convert labels to one-hot encoded representation\n","y = to_categorical(y, num_classes=10)"]},{"cell_type":"code","execution_count":60,"id":"acb382ab-e07d-4d24-82c4-bc99aeca348f","metadata":{"executionTime":6171,"lastSuccessfullyExecutedCode":"# Create the model: model\nmodel = Sequential()\n\n# Add the first hidden layer\nmodel.add(Dense(500, activation='relu', input_shape=(X.shape[1],)))\n\n# Add the second hidden layer\nmodel.add(Dense(500, activation='relu'))\n\n# Add the output layer\nmodel.add(Dense(10, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Fit the model\nmodel.fit(X, y, validation_split=0.3, epochs=10)"},"outputs":[],"source":["# Create the model: model\n","model = Sequential()\n","\n","# Add the first hidden layer\n","model.add(Dense(500, activation='relu', input_shape=(X.shape[1],)))\n","\n","# Add the second hidden layer\n","model.add(Dense(500, activation='relu'))\n","\n","# Add the output layer\n","model.add(Dense(10, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Fit the model\n","model.fit(X, y, validation_split=0.3, epochs=10)"]}],"metadata":{"editor":"DataCamp Workspace","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}
